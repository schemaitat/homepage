{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Polars confusion\n",
        "date: \"2024-07-13\"\n",
        "categories: \n",
        "  - tutorial\n",
        "  - data science\n",
        "  - dataframes\n",
        "tags: \n",
        "  - tutorial\n",
        "  - python\n",
        "  - polars\n",
        "  - data science\n",
        "  - classifier\n",
        "  - confusion\n",
        "  - optimization\n",
        "draft: false\n",
        "---"
      ],
      "id": "5ee0ce76"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "{{< admonition abstract >}}\n",
        "\n",
        "\n",
        "We explore how to use [polars](https://pola.rs/)' out of the box optimizations to\n",
        "make parallelized computations. An interesting example is the confusion\n",
        "of a classifier model and classical derived metrics like precision, recall\n",
        "or the f1-score.\n",
        "\n",
        "{{< /admonition >}}\n",
        "\n",
        "\n",
        "{{< admonition note >}}\n",
        "\n",
        "\n",
        "Polars is a blazingly fast DataFrame library implemented in Rust. It just had its 1.0 (actually 1.1) release and is a production ready tool with a stable an well designed api. \n",
        "\n",
        "{{< /admonition >}}\n",
        "\n",
        "\n",
        "   \n",
        "## Setup\n",
        "\n",
        "Let us first generate some dummy data, which consists of labels (`y_true`) and\n",
        "model scores (`y_prob`) with values in [0,1]. Furthermore we generate grouping\n",
        "variables `id_1` and `id_2`, which will later be used to partition our data.\n"
      ],
      "id": "252900ed"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import polars as pl\n",
        "import polars.selectors as cs\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(314)\n",
        "\n",
        "def generate_data(n: int) -> pl.DataFrame:\n",
        "  \"\"\"\n",
        "  Generate a DataFrame with random classifier output.\n",
        "\n",
        "  Parameters:\n",
        "    n (int): The number of rows in the DataFrame.\n",
        "\n",
        "  Returns:\n",
        "    pl.DataFrame: The generated DataFrame.\n",
        "\n",
        "  \"\"\"\n",
        "  y_true = np.random.choice([True, False], n)\n",
        "  y_prob = np.random.uniform(0, 1, n)\n",
        "\n",
        "  id_1 = np.random.choice(15, n)\n",
        "  id_2 = np.random.choice(10, n)\n",
        "\n",
        "  schema = {\n",
        "    \"id_1\": pl.Int32,\n",
        "    \"id_2\": pl.Int32,\n",
        "    \"y_true\": pl.Boolean,\n",
        "    \"y_prob\": pl.Float32,\n",
        "  }\n",
        "\n",
        "  df = pl.DataFrame(\n",
        "    {\n",
        "      \"id_1\": id_1.tolist(),\n",
        "      \"id_2\": id_2.tolist(),\n",
        "      \"y_true\": y_true.tolist(),\n",
        "      \"y_prob\": y_prob.tolist(),\n",
        "    },\n",
        "    schema=schema,\n",
        "  )\n",
        "\n",
        "  return df\n",
        "\n",
        "print(generate_data(10).head())"
      ],
      "id": "7271de9d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem description\n",
        "\n",
        "Metrics like precision, recall or the f1-score are defined entirely in terms of the\n",
        "confusion (i.e. the number true positives, false positives, true negatives and false negatives).\n",
        "These in turn are defined in terms of a threshold which defines the boolean predictions, being positive\n",
        "if and only if the score is greater than or equal to the threshold.\n",
        "\n",
        "Thus we can ask ourselves: Which threshold can we use to optimize for example the f1-score ? \n",
        "\n",
        "The most naive approach is to compute all values of the f1-score for a large enough number of thresholds\n",
        "and keep  theta for which the f1-score is maximal.\n",
        "\n",
        "The next question could be: Which thresholds can we use to optimize the f1-score on certain \n",
        "partitions of our data ?\n",
        "\n",
        "We will answer both questions at once since the first question is a special case (with a one element partition).\n",
        "\n",
        "## Working with wide dataframes\n",
        "\n",
        "An elegant solution to the problem is to use wide dataframes.\n",
        "For each theta, we generate a boolean column `y_pred(theta)` and four columns \n",
        "defining the confusion with respect to that theta. Furtermore, we add\n",
        "one more column with our metric in question, lets say the f1-score. In total\n",
        "we get 6*len(theta) extra columns.\n",
        "\n",
        "Now, the whole magic of why polars is such a great choice so solve this problem\n",
        "is that all expression will be calculated in parallel, and in addition, we \n",
        "can easily group our dataframe by our id (grouping) variables that define the\n",
        "partition. \n"
      ],
      "id": "5c0cec95"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def add_y_pred(theta: list[float]) -> dict[str, pl.Expr]:\n",
        "    \"\"\"\n",
        "    Add columns to the DataFrame with boolean predictions for different thresholds.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        f\"y_pred_{i}\" : pl.col(\"y_prob\") >= theta \n",
        "          for i, theta in enumerate(theta)\n",
        "    }\n",
        "\n",
        "def add_confusion(theta: list[float]) -> dict[str, pl.Expr]:\n",
        "    \"\"\"\n",
        "    Add columns to the DataFrame with the confusion matrix for different thresholds.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        f\"tp_{i}\" : pl.col(\"y_true\") & pl.col(f\"y_pred_{i}\")  \n",
        "          for i, theta in enumerate(theta)\n",
        "    } | {\n",
        "        f\"fp_{i}\" : ~pl.col(\"y_true\") & pl.col(f\"y_pred_{i}\") \n",
        "          for i, theta in enumerate(theta)\n",
        "    } | {\n",
        "        f\"tn_{i}\" : ~pl.col(\"y_true\") & ~pl.col(f\"y_pred_{i}\") \n",
        "          for i, theta in enumerate(theta)\n",
        "    } | {\n",
        "        f\"fn_{i}\" : pl.col(\"y_true\")  & ~pl.col(f\"y_pred_{i}\") \n",
        "          for i, theta in enumerate(theta)\n",
        "    }\n",
        "\n",
        "def add_f1_score(theta: list[float]) -> dict[str, pl.Expr]:\n",
        "    \"\"\"\n",
        "    Add columns to the DataFrame with the f1-score for different thresholds.\n",
        "    Uses the confusion matrix.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        f\"f1_score_{i}\" : 2 * pl.col(f\"tp_{i}\").sum() / (\n",
        "            2 * pl.col(f\"tp_{i}\").sum() + pl.col(f\"fp_{i}\").sum() + pl.col(f\"fn_{i}\").sum()\n",
        "          ) for i, theta in enumerate(theta)\n",
        "    }\n",
        "\n",
        "def select_best_theta(\n",
        "  df: pl.DataFrame,\n",
        "  theta: list[float],\n",
        ") -> pl.DataFrame:\n",
        "  \"\"\"\n",
        "  Select the best threshold.\n",
        "  \"\"\"\n",
        "  df_theta = pl.DataFrame({\"index\" : range(len(theta)), \"theta\" : theta},\n",
        "    schema={\n",
        "      \"index\" : pl.UInt32, \n",
        "      \"theta\" : pl.Float32,\n",
        "    })\n",
        "\n",
        "  return (\n",
        "    df\n",
        "    .with_columns(\n",
        "        theta_opt_ind = pl.concat_list(cs.starts_with(\"f1\")).list.arg_max(),\n",
        "        f1_opt = pl.concat_list(cs.starts_with(\"f1\")).list.max(),\n",
        "    )\n",
        "    .join(\n",
        "      df_theta,\n",
        "      left_on=\"theta_opt_ind\",\n",
        "      right_on=\"index\"\n",
        "    )\n",
        "    .rename({\n",
        "      \"theta\" : \"theta_opt\",\n",
        "    })\n",
        "  )\n",
        "\n",
        "\n",
        "def optimize(df: pl.DataFrame, theta: list[float], group_by: list[str]) -> pl.DataFrame:\n",
        "  \"\"\"\n",
        "  Optimize the f1-score for different thresholds on different partitions of the data.\n",
        "  \"\"\"\n",
        "  return (\n",
        "    df.with_columns(\n",
        "        **add_y_pred(theta),\n",
        "    )\n",
        "    .with_columns(\n",
        "        **add_confusion(theta),\n",
        "    )\n",
        "    .group_by(group_by)\n",
        "    .agg(\n",
        "        **add_f1_score(theta)\n",
        "    )\n",
        "    .pipe(\n",
        "      lambda df: df,\n",
        "    )\n",
        "    .select(\n",
        "        *group_by, cs.starts_with(\"f1\")\n",
        "    )\n",
        "    .pipe(\n",
        "      select_best_theta, theta\n",
        "    )\n",
        "    .select(\n",
        "        *group_by, \"theta_opt\", \"f1_opt\",\n",
        "    )\n",
        "  )"
      ],
      "id": "a3911825",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "{{< admonition example >}}\n"
      ],
      "id": "067a1097"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "theta = [0.1, 0.5]\n",
        "groups=[\"id_1\"]\n",
        "df=generate_data(100)\n",
        "\n",
        "df_wide = (\n",
        "    df.with_columns(\n",
        "        **add_y_pred(theta),\n",
        "    )\n",
        "    .with_columns(\n",
        "        **add_confusion(theta),\n",
        "    )\n",
        ")\n",
        "df_grouped = (\n",
        "  df_wide\n",
        "  .group_by(groups)\n",
        "  .agg(\n",
        "    **add_f1_score(theta)\n",
        "  )\n",
        ")\n",
        "\n",
        "df_opt = (\n",
        "  df_grouped\n",
        "  .pipe(select_best_theta, theta)\n",
        "  .select(\n",
        "    *groups, \"theta_opt_ind\", \"theta_opt\", \"f1_opt\",\n",
        "  )\n",
        ")\n",
        "\n",
        "print(df_wide.head())\n",
        "print(df_grouped.head())\n",
        "print(df_opt.head())"
      ],
      "id": "fa67ad05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "{{< /admonition >}}\n",
        "\n",
        "\n",
        "\n",
        "## More data\n",
        "Now, lets see if we can handle more data. Note that the output is generated on\n",
        "a single core machine. On my 8 core 16 GB machine, the following code runs in\n",
        "approximately 14 seconds."
      ],
      "id": "0c8b20cc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import psutil\n",
        "print(f\"CPU: {psutil.cpu_count()}\")\n",
        "print(f\"Memory: {psutil.virtual_memory().total / 1024 ** 2} MB\")"
      ],
      "id": "0007d2a0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "groups=[\"id_1\", \"id_2\"]\n",
        "df = generate_data(1_000_000)"
      ],
      "id": "91cc2210",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%time\n",
        "# use 100 equidistant thresholds\n",
        "opt = optimize(df, np.arange(0,1, 0.01), groups)\n",
        "print(opt.head(5))"
      ],
      "id": "005e1c4b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import hvplot\n",
        "hvplot.extension(\"matplotlib\")\n",
        "\n",
        "(\n",
        "  opt\n",
        "  .with_columns(f1_opt=pl.col(\"f1_opt\").round(2))\n",
        "  .plot\n",
        "  .heatmap(\"id_1\", \"id_2\", \"f1_opt\", height=600, width=800)\n",
        ")"
      ],
      "id": "af17d03d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feel free to experiment with that code or to build a benchmark using different\n",
        "techniques and tools.\n"
      ],
      "id": "a560fbdc"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}